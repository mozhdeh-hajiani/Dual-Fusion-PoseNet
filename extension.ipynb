{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOh7Dwg/IfYzDJMMeIDszCr"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11858828,"sourceType":"datasetVersion","datasetId":7451592},{"sourceId":11858862,"sourceType":"datasetVersion","datasetId":7451614},{"sourceId":11874387,"sourceType":"datasetVersion","datasetId":7462530}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F  # Import for F.mse_loss\nfrom torchvision import models\nimport cv2\nimport yaml\nfrom tqdm import tqdm","metadata":{"id":"IvgMsSv7dunm","executionInfo":{"status":"ok","timestamp":1747378718315,"user_tz":-210,"elapsed":9725,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.680251Z","iopub.execute_input":"2025-05-25T11:05:14.680933Z","iopub.status.idle":"2025-05-25T11:05:14.685329Z","shell.execute_reply.started":"2025-05-25T11:05:14.680900Z","shell.execute_reply":"2025-05-25T11:05:14.684746Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass PoseNet6D_MLP_ConcatFusion(nn.Module):\n    def __init__(self, pretrained=True, compress_rgb=True):\n        super(PoseNet6D_MLP_ConcatFusion, self).__init__()\n\n        # RGB  (ResNet50)\n        resnet_rgb = models.resnet50(\n            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n        )\n        self.rgb_backbone = nn.Sequential(*list(resnet_rgb.children())[:-1])  # (B, 2048, 1, 1)\n\n        # convert RGB features to 512-dim\n        self.compress_rgb = compress_rgb\n        if compress_rgb:\n            self.rgb_compress = nn.Linear(2048, 512)\n\n        # Depth  (ResNet18, 1-channel input)\n        resnet_depth = models.resnet18(weights=None)\n        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.depth_backbone = nn.Sequential(*list(resnet_depth.children())[:-1])  # (B, 512, 1, 1)\n\n        # 2 layer MLP  for weight\n        self.gate_mlp = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2),\n            nn.Sigmoid()\n        )\n\n        # Pose regression layers (input = 1024  concat)\n        self.fc_rot = nn.Linear(1024, 4)\n        self.fc_trans = nn.Linear(1024, 3)\n\n    def forward(self, rgb_img, depth_img):\n        # RGB feature extraction\n        rgb_feat = self.rgb_backbone(rgb_img).squeeze(-1).squeeze(-1)  # (B, 2048)\n        if self.compress_rgb:\n            rgb_feat = self.rgb_compress(rgb_feat)  # (B, 512)\n\n        # Depth feature extraction\n        depth_feat = self.depth_backbone(depth_img).squeeze(-1).squeeze(-1)  # (B, 512)\n\n        # cat\n        concat_feat = torch.cat([rgb_feat, depth_feat], dim=1)  # (B, 1024)\n        gates = self.gate_mlp(concat_feat)                      # (B, 2)\n        rgb_gate = gates[:, 0].unsqueeze(1)\n        depth_gate = gates[:, 1].unsqueeze(1)\n\n        # Apply weight\n        gated_rgb_feat = rgb_feat * rgb_gate\n        gated_depth_feat = depth_feat * depth_gate\n\n        #  concatenation (final feature = 1024)\n        fused_feat = torch.cat([gated_rgb_feat, gated_depth_feat], dim=1)  # (B, 1024)\n\n        # Pose regression\n        rot = self.fc_rot(fused_feat)\n        trans = self.fc_trans(fused_feat)\n        rot = F.normalize(rot, dim=1)\n\n        return rot, trans\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.686619Z","iopub.execute_input":"2025-05-25T11:05:14.686836Z","iopub.status.idle":"2025-05-25T11:05:14.707010Z","shell.execute_reply.started":"2025-05-25T11:05:14.686821Z","shell.execute_reply":"2025-05-25T11:05:14.706351Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass PoseNet6D_MLP_ConcatFusion_512(nn.Module):\n    def __init__(self, pretrained=True, compress_rgb=True):\n        super(PoseNet6D_MLP_ConcatFusion_512, self).__init__()\n\n        # RGB  (ResNet50)\n        resnet_rgb = models.resnet50(\n            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n        )\n        self.rgb_backbone = nn.Sequential(*list(resnet_rgb.children())[:-1])  # (B, 2048, 1, 1)\n\n        # convert RGB features to 512-dim\n        self.compress_rgb = compress_rgb\n        if compress_rgb:\n            self.rgb_compress = nn.Linear(2048, 512)\n\n        # Depth  (ResNet18, 1-channel input)\n        resnet_depth = models.resnet18(weights=None)\n        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.depth_backbone = nn.Sequential(*list(resnet_depth.children())[:-1])  # (B, 512, 1, 1)\n\n        # 2 layer MLP for weight\n        self.gate_mlp = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2),\n            nn.Sigmoid()\n        )\n\n        #  (1024 to 512)\n        self.fusion_fc = nn.Linear(1024, 512)\n\n        # Pose regression layers\n        self.fc_rot = nn.Linear(512, 4)\n        self.fc_trans = nn.Linear(512, 3)\n\n    def forward(self, rgb_img, depth_img):\n        # RGB feature extraction\n        rgb_feat = self.rgb_backbone(rgb_img).squeeze(-1).squeeze(-1)  # (B, 2048)\n        if self.compress_rgb:\n            rgb_feat = self.rgb_compress(rgb_feat)  # (B, 512)\n\n        # Depth feature extraction\n        depth_feat = self.depth_backbone(depth_img).squeeze(-1).squeeze(-1)  # (B, 512)\n\n        # cat\n        concat_feat = torch.cat([rgb_feat, depth_feat], dim=1)  # (B, 1024)\n        gates = self.gate_mlp(concat_feat)                      # (B, 2)\n        rgb_gate = gates[:, 0].unsqueeze(1)\n        depth_gate = gates[:, 1].unsqueeze(1)\n\n        # Apply weight\n        gated_rgb_feat = rgb_feat * rgb_gate\n        gated_depth_feat = depth_feat * depth_gate\n\n        #  concatenation\n        fused_feat = torch.cat([gated_rgb_feat, gated_depth_feat], dim=1)  # (B, 1024)\n        fused_feat = self.fusion_fc(fused_feat)  # (B, 512)\n\n        # Pose regression\n        rot = self.fc_rot(fused_feat)\n        trans = self.fc_trans(fused_feat)\n        rot = F.normalize(rot, dim=1)\n\n        return rot, trans\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.707706Z","iopub.execute_input":"2025-05-25T11:05:14.707943Z","iopub.status.idle":"2025-05-25T11:05:14.737082Z","shell.execute_reply.started":"2025-05-25T11:05:14.707918Z","shell.execute_reply":"2025-05-25T11:05:14.736403Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass PoseNet6D_MLP_Fusion(nn.Module):\n    def __init__(self, pretrained=True, compress_rgb=True):\n        super(PoseNet6D_MLP_Fusion, self).__init__()\n\n        # RGB  (ResNet50)\n        resnet_rgb = models.resnet50(\n            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n        )\n        self.rgb_backbone = nn.Sequential(*list(resnet_rgb.children())[:-1])  # (B, 2048, 1, 1)\n\n        # convert to 512-dim\n        self.compress_rgb = compress_rgb\n        if compress_rgb:\n            self.rgb_compress = nn.Linear(2048, 512)\n\n        # Depth  (ResNet18, 1-channel input)\n        resnet_depth = models.resnet18(weights=None)\n        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.depth_backbone = nn.Sequential(*list(resnet_depth.children())[:-1])  # (B, 512, 1, 1)\n\n        # 2layer MLP \n        self.gate_mlp = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2),      #  one for RGB, one for Depth\n            nn.Sigmoid()            # Output in [0,1]\n        )\n\n        # Pose regression layers \n        self.fc_rot = nn.Linear(512, 4)   # quaternion\n        self.fc_trans = nn.Linear(512, 3) # translation\n\n    def forward(self, rgb_img, depth_img):\n        # RGB feature extraction\n        rgb_feat = self.rgb_backbone(rgb_img).squeeze(-1).squeeze(-1)  # (B, 2048)\n        if self.compress_rgb:\n            rgb_feat = self.rgb_compress(rgb_feat)  # (B, 512)\n\n        # Depth feature extraction\n        depth_feat = self.depth_backbone(depth_img).squeeze(-1).squeeze(-1)  # (B, 512)\n\n        # Concatenate features\n        concat_feat = torch.cat([rgb_feat, depth_feat], dim=1)  # (B, 1024)\n        gates = self.gate_mlp(concat_feat)                      # (B, 2), values in [0,1]\n\n        # Split gating weights\n        rgb_gate = gates[:, 0].unsqueeze(1)     # (B,1)\n        depth_gate = gates[:, 1].unsqueeze(1)   # (B,1)\n\n        # element-wise multiply\n        gated_rgb_feat = rgb_feat * rgb_gate    # (B, 512)\n        gated_depth_feat = depth_feat * depth_gate  # (B, 512)\n\n        #  sum\n        fused_feat = gated_rgb_feat + gated_depth_feat  # (B, 512)\n\n        # Pose regression\n        rot = self.fc_rot(fused_feat)       # (B, 4)\n        trans = self.fc_trans(fused_feat)   # (B, 3)\n\n        # Normalize quaternion to unit length\n        rot = F.normalize(rot, dim=1)\n\n        return rot, trans\n","metadata":{"id":"LS1C2R9O2qog","executionInfo":{"status":"ok","timestamp":1747378718375,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.787743Z","iopub.execute_input":"2025-05-25T11:05:14.787943Z","iopub.status.idle":"2025-05-25T11:05:14.796048Z","shell.execute_reply.started":"2025-05-25T11:05:14.787929Z","shell.execute_reply":"2025-05-25T11:05:14.795354Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport cv2\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport torchvision.transforms as T\nimport os\nimport yaml\nfrom PIL import Image\n\n\nclass PoseDataset(Dataset):\n    def __init__(self,  rgb_dir, depth_dir,linemod_root, augment=False):\n\n        self.rgb_dir = rgb_dir\n        self.depth_dir = depth_dir\n        self.linemod_root = linemod_root\n        self.RGB_img_filenames = sorted([\n            f for f in os.listdir(rgb_dir) if f.endswith(\".png\")\n        ])\n        self.depth_img_filenames = sorted([\n            f for f in os.listdir(depth_dir) if f.endswith(\".png\")\n        ])\n\n\n        # Preload gt.yml data for all classes\n        self.gt_data = {}\n        for class_id in range(1, 16):\n            class_str = f\"{class_id:02d}\"\n            gt_path = os.path.join(linemod_root, class_str, \"gt.yml\")\n            if os.path.exists(gt_path):\n                with open(gt_path, 'r') as f:\n                    self.gt_data[class_str] = yaml.safe_load(f)\n\n        \n        self.rgb_transform = T.Compose([\n            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            T.RandomHorizontalFlip(),\n            T.Resize((224, 224)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225])\n        ])\n        self.depth_transform = T.Compose([\n            # T.ToTensor(),\n            # T.Resize((224, 224)),\n            T.Resize((224, 224)),\n            T.ToTensor()  # Converts PIL float32 to FloatTensor [1, H, W]\n        ])\n\n    def __len__(self):\n        return len(self.RGB_img_filenames)\n\n    def normalize_depth(self,depth):\n        depth = np.array(depth).astype(np.float32)\n        return (depth - depth.min()) / (depth.max() - depth.min() + 1e-8)\n\n    def __getitem__(self, idx):\n\n        RGB_filename = self.RGB_img_filenames[idx]\n        class_id_str, img_id_str = RGB_filename.split(\"_\")\n        img_id = int(os.path.splitext(img_id_str)[0])\n\n        # Load RGB image\n        rgb_path = os.path.join(self.rgb_dir, RGB_filename)\n        rgb_img = Image.open(rgb_path).convert(\"RGB\")\n        rgb_tensor = self.rgb_transform(rgb_img)\n\n        # Load Depth image\n        # depth_path = os.path.join(self.depth_dir, RGB_filename)\n        # depth_img = Image.open(depth_path).convert(\"I\")   # Single-channel\n\n        # # Normalize and convert to PIL for transforms\n        # depth_np_norm = self.normalize_depth(depth_img)\n        # depth_img_norm = Image.fromarray((depth_np_norm * 255).astype(np.uint8))\n\n        # depth_tensor = self.depth_transform(depth_img_norm)\n           # Load Depth image (PIL single channel)\n        depth_path = os.path.join(self.depth_dir, RGB_filename)\n        depth_img = Image.open(depth_path).convert(\"I\")  # 32-bit integer depth\n\n        # Convert to float numpy, normalize (e.g., scale mm->meters or divide by max)\n        depth_np = np.array(depth_img).astype(np.float32)\n        depth_np /= 1000.0  # if in mm, convert to meters, adjust as per your data\n\n        # Optional: clip depth values to a max distance (e.g., 2 meters)\n        depth_np = np.clip(depth_np, 0, 2.0)\n\n        # Normalize depth to [0,1] by dividing by max depth value (2.0)\n        depth_np /= 2.0\n\n        # Convert normalized float depth to PIL image in 'F' mode (32-bit float)\n        depth_img_float = Image.fromarray(depth_np).convert('F')\n\n        # Apply depth transforms (Resize -> ToTensor)\n        depth_tensor = self.depth_transform(depth_img_float)  # [1, H, W], float32 in [0,1]\n\n\n        # Load pose from GT file\n        pose_list = self.gt_data[class_id_str][img_id]\n        pose = next(item for item in pose_list if item['obj_id'] == int(class_id_str))\n\n        R_mat = np.array(pose['cam_R_m2c']).reshape(3, 3).astype(np.float32)\n        quat = R.from_matrix(R_mat).as_quat().astype(np.float32)\n        quat /= np.linalg.norm(quat)\n        t_vec = np.array(pose['cam_t_m2c'], dtype=np.float32) / 1000.0  #  to meters\n\n        return {\n            'RGB_image': rgb_tensor,\n            'depth_image': depth_tensor,\n            'rotation': torch.tensor(quat, dtype=torch.float32),\n            'rotation_matrix': torch.tensor(R_mat, dtype=torch.float32),\n            'translation': torch.tensor(t_vec, dtype=torch.float32),\n            'class_id': int(class_id_str),\n            'filename': RGB_filename\n        }\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.814552Z","iopub.execute_input":"2025-05-25T11:05:14.814741Z","iopub.status.idle":"2025-05-25T11:05:14.828761Z","shell.execute_reply.started":"2025-05-25T11:05:14.814726Z","shell.execute_reply":"2025-05-25T11:05:14.828091Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass PoseNet6D(nn.Module):\n    def __init__(self, pretrained=True, compress_rgb=True):\n        super(PoseNet6D, self).__init__()\n\n        # ==== RGB branch: ResNet50 ====\n        resnet_rgb = models.resnet50(\n            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n        )\n        self.rgb_backbone = nn.Sequential(*list(resnet_rgb.children())[:-1])  # Output: (B, 2048, 1, 1)\n\n        #  compression layer for RGB features\n        self.compress_rgb = compress_rgb\n        if compress_rgb:\n            self.rgb_compress = nn.Linear(2048, 512)  # Match depth feature dim\n\n        # ==== Depth branch: ResNet18 modified for 1-channel input ====\n        resnet_depth = models.resnet18(weights=None)  # No pretrained weights for depth\n        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.depth_backbone = nn.Sequential(*list(resnet_depth.children())[:-1])  # Output: (B, 512, 1, 1)\n\n        # ==== Pose regression ====\n        self.fc_rot = nn.Linear(1024, 4)   # 512 + 512 -> Quaternion\n        self.fc_trans = nn.Linear(1024, 3) # 512 + 512 -> Translation\n\n    def forward(self, rgb_img, depth_img):\n        # RGB features\n        rgb_feat = self.rgb_backbone(rgb_img).squeeze(-1).squeeze(-1)  # (B, 2048)\n        if self.compress_rgb:\n            rgb_feat = self.rgb_compress(rgb_feat)  # (B, 512)\n\n        # Depth features\n        depth_feat = self.depth_backbone(depth_img).squeeze(-1).squeeze(-1)  # (B, 512)\n\n        # Concatenate features\n        feat = torch.cat([rgb_feat, depth_feat], dim=1)  # (B, 1024)\n\n        # Predict pose\n        rot = self.fc_rot(feat)         # (B, 4)\n        trans = self.fc_trans(feat)     # (B, 3)\n        rot = F.normalize(rot, dim=1)   # Normalize quaternion\n        return rot, trans\n","metadata":{"id":"IUcsF6Mrd4zO","executionInfo":{"status":"ok","timestamp":1747378718398,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.829758Z","iopub.execute_input":"2025-05-25T11:05:14.829933Z","iopub.status.idle":"2025-05-25T11:05:14.848101Z","shell.execute_reply.started":"2025-05-25T11:05:14.829919Z","shell.execute_reply":"2025-05-25T11:05:14.847370Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef mse_pose_loss(pred_q, pred_t, gt_q, gt_t):\n    return torch.mean((pred_q - gt_q)**2) + torch.mean((pred_t - gt_t)**2)\n\ndef angle_pose_loss(pred_q, pred_t, gt_q, gt_t):\n    pred_q = F.normalize(pred_q, dim=1)\n    gt_q = F.normalize(gt_q, dim=1)\n    cos_sim = torch.sum(pred_q * gt_q, dim=1).clamp(-1+1e-7, 1-1e-7)\n    angle_loss = torch.mean(1 - cos_sim.abs())\n    trans_loss = torch.mean((pred_t - gt_t)**2)\n    return angle_loss + trans_loss\n\ndef smooth_l1_pose_loss(pred_q, pred_t, gt_q, gt_t):\n    return F.smooth_l1_loss(pred_q, gt_q) + F.smooth_l1_loss(pred_t, gt_t)\n\ndef pose_loss(pred_q, pred_t, gt_q, gt_t):\n    rot_loss = 1 - torch.sum(pred_q * gt_q, dim=1)**2\n    trans_loss = torch.mean((pred_t - gt_t)**2, dim=1)\n    return rot_loss.mean() + trans_loss.mean()\n","metadata":{"id":"EacvZ1oNRRbE","executionInfo":{"status":"ok","timestamp":1747378718412,"user_tz":-210,"elapsed":4,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.848938Z","iopub.execute_input":"2025-05-25T11:05:14.849723Z","iopub.status.idle":"2025-05-25T11:05:14.866635Z","shell.execute_reply.started":"2025-05-25T11:05:14.849707Z","shell.execute_reply":"2025-05-25T11:05:14.865880Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, device, scaler=None):\n    model.train()\n    total_loss = 0.0\n\n    for batch in dataloader:\n        rgb = batch['RGB_image'].to(device)\n        depth = batch['depth_image'].to(device)\n        gt_q = batch['rotation'].to(device)\n        gt_t = batch['translation'].to(device)\n\n        optimizer.zero_grad()\n\n        if scaler:  # Mixed precision\n            with torch.cuda.amp.autocast():\n                pred_q, pred_t = model(rgb, depth)\n                # print(f\"pred_q={pred_q} and pred_t={pred_t}\")\n                # print(f\"gt_q={gt_q} and gt_t={gt_t}\")\n                # loss = pose_loss(pred_q, pred_t, gt_q, gt_t)\n                # loss = smooth_l1_pose_loss(pred_q, pred_t, gt_q, gt_t)\n                loss = angle_pose_loss(pred_q, pred_t, gt_q, gt_t)\n                # loss = mse_pose_loss(pred_q, pred_t, gt_q, gt_t)\n                # print(f\"losss={loss}\")\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            pred_q, pred_t = model(rgb, depth)\n            # print(f\"pred_q={pred_q} and pred_t={pred_t}\")\n            # print(f\"gt_q={gt_q} and gt_t={gt_t}\")\n            # loss = pose_loss(pred_q, pred_t, gt_q, gt_t)\n            # loss = smooth_l1_pose_loss(pred_q, pred_t, gt_q, gt_t)\n            loss = angle_pose_loss(pred_q, pred_t, gt_q, gt_t)\n            # loss = mse_pose_loss(pred_q, pred_t, gt_q, gt_t)\n            # print(f\"losss={loss}\")\n               \n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional\n            optimizer.step()\n        \n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n","metadata":{"id":"r8uyfh_b84Sl","executionInfo":{"status":"ok","timestamp":1747378718417,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.868257Z","iopub.execute_input":"2025-05-25T11:05:14.868488Z","iopub.status.idle":"2025-05-25T11:05:14.886469Z","shell.execute_reply.started":"2025-05-25T11:05:14.868471Z","shell.execute_reply":"2025-05-25T11:05:14.885914Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def validate_model(model, dataloader, device):\n    model.eval()\n    total_loss = 0.0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            rgb = batch['RGB_image'].to(device)\n            depth = batch['depth_image'].to(device)\n            gt_q = batch['rotation'].to(device)\n            gt_t = batch['translation'].to(device)\n\n            pred_q, pred_t = model(rgb, depth)\n            # loss = pose_loss(pred_q, pred_t, gt_q, gt_t)\n            # loss = smooth_l1_pose_loss(pred_q, pred_t, gt_q, gt_t)\n            loss = angle_pose_loss(pred_q, pred_t, gt_q, gt_t)\n            # loss = mse_pose_loss(pred_q, pred_t, gt_q, gt_t)\n            total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n","metadata":{"id":"poq9Eb3J88E3","executionInfo":{"status":"ok","timestamp":1747378718422,"user_tz":-210,"elapsed":2,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.887280Z","iopub.execute_input":"2025-05-25T11:05:14.887575Z","iopub.status.idle":"2025-05-25T11:05:14.905989Z","shell.execute_reply.started":"2025-05-25T11:05:14.887560Z","shell.execute_reply":"2025-05-25T11:05:14.905308Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"# ###test different optimiser using validation\n\n# import os\n# import random\n# import shutil\n# from sklearn.model_selection import train_test_split\n# from torch.utils.data import DataLoader\n# import torch\n# import torch.nn as nn\n# from torch.optim.lr_scheduler import ReduceLROnPlateau\n# # ----- Paths -----\n# RGB_cropped_dir = \"/kaggle/input/rgboutput/RGB_crop/train/train_cropped_objects\"\n# depth_cropped_dir = \"/kaggle/input/depthoutput/depth_crop/train/train_cropped_objects\"\n# linemod_root = \"/kaggle/input/linemod/Linemod_preprocessed/data\"\n# working_dir = \"/kaggle/working\"\n# RGB_train_dir = os.path.join(working_dir, \"RGB/train\")\n# RGB_val_dir = os.path.join(working_dir, \"RGB/val\")\n# depth_train_dir = os.path.join(working_dir, \"depth/train\")\n# depth_val_dir = os.path.join(working_dir, \"depth/val\")\n# split_ratio = 0.8\n\n# os.makedirs(RGB_train_dir, exist_ok=True)\n# os.makedirs(RGB_val_dir, exist_ok=True)\n# os.makedirs(depth_train_dir, exist_ok=True)\n# os.makedirs(depth_val_dir, exist_ok=True)\n\n# image_files = [f for f in os.listdir(RGB_cropped_dir) if f.endswith(\".png\")]\n# train_files, val_files = train_test_split(image_files, train_size=split_ratio, random_state=42)\n\n# for file in train_files:\n#     shutil.copy(os.path.join(RGB_cropped_dir, file), os.path.join(RGB_train_dir, file))\n#     shutil.copy(os.path.join(depth_cropped_dir, file), os.path.join(depth_train_dir, file))\n\n# for file in val_files:\n#     shutil.copy(os.path.join(RGB_cropped_dir, file), os.path.join(RGB_val_dir, file))\n#     shutil.copy(os.path.join(depth_cropped_dir, file), os.path.join(depth_val_dir, file))\n\n# # ----- Load datasets -----\n# train_dataset = PoseDataset(rgb_dir=RGB_train_dir, depth_dir=depth_train_dir, linemod_root=linemod_root)\n# val_dataset = PoseDataset(rgb_dir=RGB_val_dir, depth_dir=depth_val_dir, linemod_root=linemod_root)\n# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n# val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# # ----- Model -----\n# model = PoseNet6D_MLP_Fusion(pretrained=True).to(device)   # PoseNet6D_MLP_Fusion  PoseNet6D_MLP_ConcatFusion PoseNet6D_MLP_ConcatFusion_512 \n\n# # ----- Optimizer -----\n# optimizer_type = \"SGD\"  # Choice: \"SGD\", \"Adam\", \"AdamW\", \"RMSprop\"\n# learning_rate = 1e-4\n\n# if optimizer_type == \"SGD\":\n#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n# elif optimizer_type == \"Adam\":\n#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n# elif optimizer_type == \"AdamW\":\n#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n# elif optimizer_type == \"RMSprop\":\n#     optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n# else:\n#     raise ValueError(f\"Unsupported optimizer: {optimizer_type}\")\n\n# # ----- Save Paths -----\n# save_path = os.path.join(working_dir, \"extension_model\")\n# os.makedirs(save_path, exist_ok=True)\n# best_model_path = os.path.join(save_path, \"best_model.pth\")\n# checkpoint_path = os.path.join(save_path, \"checkpoint.pth\")\n\n# # ----- Resume checkpoint -----\n# start_epoch = 0\n# best_val_loss = float('inf')\n# if os.path.exists(checkpoint_path):\n#     checkpoint = torch.load(checkpoint_path)\n#     model.load_state_dict(checkpoint['model_state_dict'])\n#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n#     best_val_loss = checkpoint['best_val_loss']\n#     start_epoch = checkpoint['epoch'] + 1\n#     print(f\"Resumed training from epoch {start_epoch}\")\n# else:\n#     print(\"No checkpoint found, starting from epoch 0\")\n\n# # ----- Training Loop -----\n# patience = 20\n# no_improve_counter = 0\n# epoch_num = 130\n# scaler = torch.cuda.amp.GradScaler()\n\n# for epoch in range(start_epoch, epoch_num):\n#     print(f\"\\nEpoch {epoch + 1}/{epoch_num}\")\n\n#     train_loss = train_model(model, train_loader, optimizer, device, scaler=scaler)\n#     val_loss = validate_model(model, val_loader, device)\n\n#     print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n\n#     if val_loss < best_val_loss:\n#         best_val_loss = val_loss\n#         torch.save(model.state_dict(), best_model_path)\n#         print(f\"✅ Saved new best model at epoch {epoch + 1} with val loss {val_loss:.4f}\")\n#         no_improve_counter = 0\n#     # else:\n#     #     no_improve_counter += 1\n#     #     if no_improve_counter >= patience:\n#     #         print(\"⏹️ Early stopping triggered\")\n#     #         break\n\n#     checkpoint = {\n#         'epoch': epoch,\n#         'model_state_dict': model.state_dict(),\n#         'optimizer_state_dict': optimizer.state_dict(),\n#         'best_val_loss': best_val_loss\n#     }\n#     torch.save(checkpoint, checkpoint_path)\n#     print(f\"Checkpoint saved at epoch {epoch + 1}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.906800Z","iopub.execute_input":"2025-05-25T11:05:14.907291Z","iopub.status.idle":"2025-05-25T11:05:14.925458Z","shell.execute_reply.started":"2025-05-25T11:05:14.907275Z","shell.execute_reply":"2025-05-25T11:05:14.924826Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"\n###test different learning rate using validation\nimport os\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\n\n\n# Paths \nRGB_cropped_dir = \"/kaggle/input/rgboutput/RGB_crop/train/train_cropped_objects\"\ndepth_cropped_dir = \"/kaggle/input/depthoutput/depth_crop/train/train_cropped_objects\"\nlinemod_root = \"/kaggle/input/linemod/Linemod_preprocessed/data\"\n\n\nworking_dir = \"/kaggle/working\"\nRGB_train_dir = os.path.join(working_dir, \"RGB/train\")\nRGB_val_dir = os.path.join(working_dir, \"RGB/val\")\ndepth_train_dir = os.path.join(working_dir, \"depth/train\")\ndepth_val_dir = os.path.join(working_dir, \"depth/val\")\nsplit_ratio = 0.8\n\n# Create folders\nos.makedirs(RGB_train_dir, exist_ok=True)\nos.makedirs(RGB_val_dir, exist_ok=True)\nos.makedirs(depth_train_dir, exist_ok=True)\nos.makedirs(depth_val_dir, exist_ok=True)\n\n# Get image list and split\nimage_files = [f for f in os.listdir(RGB_cropped_dir) if f.endswith(\".png\")]\ntrain_files, val_files = train_test_split(image_files, train_size=split_ratio, random_state=42)\n\n# Copy data to train/val folders\nfor file in train_files:\n    shutil.copy(os.path.join(RGB_cropped_dir, file), os.path.join(RGB_train_dir, file))\n    shutil.copy(os.path.join(depth_cropped_dir, file), os.path.join(depth_train_dir, file))\n\nfor file in val_files:\n    shutil.copy(os.path.join(RGB_cropped_dir, file), os.path.join(RGB_val_dir, file))\n    shutil.copy(os.path.join(depth_cropped_dir, file), os.path.join(depth_val_dir, file))\n\n# Load datasets\ntrain_dataset = PoseDataset(rgb_dir=RGB_train_dir, depth_dir=depth_train_dir, linemod_root=linemod_root)\nval_dataset = PoseDataset(rgb_dir=RGB_val_dir, depth_dir=depth_val_dir, linemod_root=linemod_root)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Initialize model\nmodel = PoseNet6D_MLP_ConcatFusion(pretrained=True).to(device)   #PoseNet6D_MLP_Fusion PoseNet6D_MLP_ConcatFusion   \noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\nscheduler = ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.5, verbose=True)\n\n# Save directory\nsave_path = os.path.join(working_dir, \"extension_model\")\nos.makedirs(save_path, exist_ok=True)\nbest_model_path = os.path.join(save_path, \"best_model.pth\")\ncheckpoint_path = os.path.join(save_path, \"checkpoint.pth\")\n\n# Load checkpoint if exists\nstart_epoch = 0\nbest_val_loss = float('inf')\nif os.path.exists(checkpoint_path):\n    checkpoint = torch.load(checkpoint_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    best_val_loss = checkpoint['best_val_loss']\n    start_epoch = checkpoint['epoch'] + 1\n    print(f\"Resumed training from epoch {start_epoch}\")\nelse:\n    print(\"No checkpoint found, starting from epoch 0\")\n\n# Training loop\npatience = 20\nno_improve_counter = 0\nepoch_num = 100\n\nfor epoch in range(start_epoch, epoch_num):\n    print(f\"\\nEpoch {epoch + 1}/{epoch_num}\")\n\n    train_loss = train_model(model, train_loader, optimizer, device)\n    val_loss = validate_model(model, val_loader, device)\n\n    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n    scheduler.step(val_loss)\n\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), best_model_path)\n        print(f\"✅ Saved new best model at epoch {epoch + 1} with val loss {val_loss:.4f}\")\n\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'best_val_loss': best_val_loss\n    }\n    torch.save(checkpoint, checkpoint_path)\n    print(f\"Checkpoint saved at epoch {epoch + 1}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:05:14.926096Z","iopub.execute_input":"2025-05-25T11:05:14.926255Z","iopub.status.idle":"2025-05-25T11:47:56.439880Z","shell.execute_reply.started":"2025-05-25T11:05:14.926242Z","shell.execute_reply":"2025-05-25T11:47:56.438918Z"}},"outputs":[{"name":"stdout","text":"No checkpoint found, starting from epoch 0\n\nEpoch 1/100\nTrain Loss: 0.3716, Validation Loss: 0.3217\n✅ Saved new best model at epoch 1 with val loss 0.3217\nCheckpoint saved at epoch 1\n\nEpoch 2/100\nTrain Loss: 0.2667, Validation Loss: 0.2679\n✅ Saved new best model at epoch 2 with val loss 0.2679\nCheckpoint saved at epoch 2\n\nEpoch 3/100\nTrain Loss: 0.2212, Validation Loss: 0.2319\n✅ Saved new best model at epoch 3 with val loss 0.2319\nCheckpoint saved at epoch 3\n\nEpoch 4/100\nTrain Loss: 0.1943, Validation Loss: 0.2119\n✅ Saved new best model at epoch 4 with val loss 0.2119\nCheckpoint saved at epoch 4\n\nEpoch 5/100\nTrain Loss: 0.1871, Validation Loss: 0.2068\n✅ Saved new best model at epoch 5 with val loss 0.2068\nCheckpoint saved at epoch 5\n\nEpoch 6/100\nTrain Loss: 0.1594, Validation Loss: 0.1818\n✅ Saved new best model at epoch 6 with val loss 0.1818\nCheckpoint saved at epoch 6\n\nEpoch 7/100\nTrain Loss: 0.1541, Validation Loss: 0.1537\n✅ Saved new best model at epoch 7 with val loss 0.1537\nCheckpoint saved at epoch 7\n\nEpoch 8/100\nTrain Loss: 0.1406, Validation Loss: 0.1496\n✅ Saved new best model at epoch 8 with val loss 0.1496\nCheckpoint saved at epoch 8\n\nEpoch 9/100\nTrain Loss: 0.1187, Validation Loss: 0.1158\n✅ Saved new best model at epoch 9 with val loss 0.1158\nCheckpoint saved at epoch 9\n\nEpoch 10/100\nTrain Loss: 0.0994, Validation Loss: 0.1116\n✅ Saved new best model at epoch 10 with val loss 0.1116\nCheckpoint saved at epoch 10\n\nEpoch 11/100\nTrain Loss: 0.0896, Validation Loss: 0.1098\n✅ Saved new best model at epoch 11 with val loss 0.1098\nCheckpoint saved at epoch 11\n\nEpoch 12/100\nTrain Loss: 0.0700, Validation Loss: 0.0998\n✅ Saved new best model at epoch 12 with val loss 0.0998\nCheckpoint saved at epoch 12\n\nEpoch 13/100\nTrain Loss: 0.0660, Validation Loss: 0.0758\n✅ Saved new best model at epoch 13 with val loss 0.0758\nCheckpoint saved at epoch 13\n\nEpoch 14/100\nTrain Loss: 0.0540, Validation Loss: 0.0780\nCheckpoint saved at epoch 14\n\nEpoch 15/100\nTrain Loss: 0.0538, Validation Loss: 0.0736\n✅ Saved new best model at epoch 15 with val loss 0.0736\nCheckpoint saved at epoch 15\n\nEpoch 16/100\nTrain Loss: 0.0410, Validation Loss: 0.0720\n✅ Saved new best model at epoch 16 with val loss 0.0720\nCheckpoint saved at epoch 16\n\nEpoch 17/100\nTrain Loss: 0.0343, Validation Loss: 0.0645\n✅ Saved new best model at epoch 17 with val loss 0.0645\nCheckpoint saved at epoch 17\n\nEpoch 18/100\nTrain Loss: 0.0304, Validation Loss: 0.0557\n✅ Saved new best model at epoch 18 with val loss 0.0557\nCheckpoint saved at epoch 18\n\nEpoch 19/100\nTrain Loss: 0.0292, Validation Loss: 0.0631\nCheckpoint saved at epoch 19\n\nEpoch 20/100\nTrain Loss: 0.0305, Validation Loss: 0.0616\nCheckpoint saved at epoch 20\n\nEpoch 21/100\nTrain Loss: 0.0314, Validation Loss: 0.0651\nCheckpoint saved at epoch 21\n\nEpoch 22/100\nTrain Loss: 0.0274, Validation Loss: 0.0568\nCheckpoint saved at epoch 22\n\nEpoch 23/100\nTrain Loss: 0.0272, Validation Loss: 0.0567\nCheckpoint saved at epoch 23\n\nEpoch 24/100\nTrain Loss: 0.0235, Validation Loss: 0.0551\n✅ Saved new best model at epoch 24 with val loss 0.0551\nCheckpoint saved at epoch 24\n\nEpoch 25/100\nTrain Loss: 0.0188, Validation Loss: 0.0495\n✅ Saved new best model at epoch 25 with val loss 0.0495\nCheckpoint saved at epoch 25\n\nEpoch 26/100\nTrain Loss: 0.0207, Validation Loss: 0.0506\nCheckpoint saved at epoch 26\n\nEpoch 27/100\nTrain Loss: 0.0158, Validation Loss: 0.0441\n✅ Saved new best model at epoch 27 with val loss 0.0441\nCheckpoint saved at epoch 27\n\nEpoch 28/100\nTrain Loss: 0.0163, Validation Loss: 0.0467\nCheckpoint saved at epoch 28\n\nEpoch 29/100\nTrain Loss: 0.0120, Validation Loss: 0.0421\n✅ Saved new best model at epoch 29 with val loss 0.0421\nCheckpoint saved at epoch 29\n\nEpoch 30/100\nTrain Loss: 0.0148, Validation Loss: 0.0407\n✅ Saved new best model at epoch 30 with val loss 0.0407\nCheckpoint saved at epoch 30\n\nEpoch 31/100\nTrain Loss: 0.0124, Validation Loss: 0.0445\nCheckpoint saved at epoch 31\n\nEpoch 32/100\nTrain Loss: 0.0096, Validation Loss: 0.0436\nCheckpoint saved at epoch 32\n\nEpoch 33/100\nTrain Loss: 0.0100, Validation Loss: 0.0405\n✅ Saved new best model at epoch 33 with val loss 0.0405\nCheckpoint saved at epoch 33\n\nEpoch 34/100\nTrain Loss: 0.0088, Validation Loss: 0.0372\n✅ Saved new best model at epoch 34 with val loss 0.0372\nCheckpoint saved at epoch 34\n\nEpoch 35/100\nTrain Loss: 0.0094, Validation Loss: 0.0542\nCheckpoint saved at epoch 35\n\nEpoch 36/100\nTrain Loss: 0.0118, Validation Loss: 0.0467\nCheckpoint saved at epoch 36\n\nEpoch 37/100\nTrain Loss: 0.0095, Validation Loss: 0.0392\nCheckpoint saved at epoch 37\n\nEpoch 38/100\nTrain Loss: 0.0087, Validation Loss: 0.0394\nCheckpoint saved at epoch 38\n\nEpoch 39/100\nTrain Loss: 0.0078, Validation Loss: 0.0418\nCheckpoint saved at epoch 39\n\nEpoch 40/100\nTrain Loss: 0.0076, Validation Loss: 0.0362\n✅ Saved new best model at epoch 40 with val loss 0.0362\nCheckpoint saved at epoch 40\n\nEpoch 41/100\nTrain Loss: 0.0063, Validation Loss: 0.0322\n✅ Saved new best model at epoch 41 with val loss 0.0322\nCheckpoint saved at epoch 41\n\nEpoch 42/100\nTrain Loss: 0.0060, Validation Loss: 0.0376\nCheckpoint saved at epoch 42\n\nEpoch 43/100\nTrain Loss: 0.0056, Validation Loss: 0.0320\n✅ Saved new best model at epoch 43 with val loss 0.0320\nCheckpoint saved at epoch 43\n\nEpoch 44/100\nTrain Loss: 0.0058, Validation Loss: 0.0326\nCheckpoint saved at epoch 44\n\nEpoch 45/100\nTrain Loss: 0.0053, Validation Loss: 0.0329\nCheckpoint saved at epoch 45\n\nEpoch 46/100\nTrain Loss: 0.0079, Validation Loss: 0.0391\nCheckpoint saved at epoch 46\n\nEpoch 47/100\nTrain Loss: 0.0059, Validation Loss: 0.0368\nCheckpoint saved at epoch 47\n\nEpoch 48/100\nTrain Loss: 0.0054, Validation Loss: 0.0339\nCheckpoint saved at epoch 48\n\nEpoch 49/100\nTrain Loss: 0.0117, Validation Loss: 0.0415\nCheckpoint saved at epoch 49\n\nEpoch 50/100\nTrain Loss: 0.0085, Validation Loss: 0.0404\nCheckpoint saved at epoch 50\n\nEpoch 51/100\nTrain Loss: 0.0053, Validation Loss: 0.0388\nCheckpoint saved at epoch 51\n\nEpoch 52/100\nTrain Loss: 0.0059, Validation Loss: 0.0337\nCheckpoint saved at epoch 52\n\nEpoch 53/100\nTrain Loss: 0.0058, Validation Loss: 0.0393\nCheckpoint saved at epoch 53\n\nEpoch 54/100\nTrain Loss: 0.0041, Validation Loss: 0.0306\n✅ Saved new best model at epoch 54 with val loss 0.0306\nCheckpoint saved at epoch 54\n\nEpoch 55/100\nTrain Loss: 0.0043, Validation Loss: 0.0337\nCheckpoint saved at epoch 55\n\nEpoch 56/100\nTrain Loss: 0.0036, Validation Loss: 0.0332\nCheckpoint saved at epoch 56\n\nEpoch 57/100\nTrain Loss: 0.0034, Validation Loss: 0.0343\nCheckpoint saved at epoch 57\n\nEpoch 58/100\nTrain Loss: 0.0037, Validation Loss: 0.0301\n✅ Saved new best model at epoch 58 with val loss 0.0301\nCheckpoint saved at epoch 58\n\nEpoch 59/100\nTrain Loss: 0.0030, Validation Loss: 0.0281\n✅ Saved new best model at epoch 59 with val loss 0.0281\nCheckpoint saved at epoch 59\n\nEpoch 60/100\nTrain Loss: 0.0029, Validation Loss: 0.0327\nCheckpoint saved at epoch 60\n\nEpoch 61/100\nTrain Loss: 0.0027, Validation Loss: 0.0324\nCheckpoint saved at epoch 61\n\nEpoch 62/100\nTrain Loss: 0.0023, Validation Loss: 0.0316\nCheckpoint saved at epoch 62\n\nEpoch 63/100\nTrain Loss: 0.0022, Validation Loss: 0.0341\nCheckpoint saved at epoch 63\n\nEpoch 64/100\nTrain Loss: 0.0023, Validation Loss: 0.0312\nCheckpoint saved at epoch 64\n\nEpoch 65/100\nTrain Loss: 0.0022, Validation Loss: 0.0288\nCheckpoint saved at epoch 65\n\nEpoch 66/100\nTrain Loss: 0.0022, Validation Loss: 0.0320\nCheckpoint saved at epoch 66\n\nEpoch 67/100\nTrain Loss: 0.0018, Validation Loss: 0.0317\nCheckpoint saved at epoch 67\n\nEpoch 68/100\nTrain Loss: 0.0017, Validation Loss: 0.0279\n✅ Saved new best model at epoch 68 with val loss 0.0279\nCheckpoint saved at epoch 68\n\nEpoch 69/100\nTrain Loss: 0.0016, Validation Loss: 0.0307\nCheckpoint saved at epoch 69\n\nEpoch 70/100\nTrain Loss: 0.0015, Validation Loss: 0.0294\nCheckpoint saved at epoch 70\n\nEpoch 71/100\nTrain Loss: 0.0015, Validation Loss: 0.0298\nCheckpoint saved at epoch 71\n\nEpoch 72/100\nTrain Loss: 0.0016, Validation Loss: 0.0318\nCheckpoint saved at epoch 72\n\nEpoch 73/100\nTrain Loss: 0.0015, Validation Loss: 0.0332\nCheckpoint saved at epoch 73\n\nEpoch 74/100\nTrain Loss: 0.0014, Validation Loss: 0.0297\nCheckpoint saved at epoch 74\n\nEpoch 75/100\nTrain Loss: 0.0013, Validation Loss: 0.0291\nCheckpoint saved at epoch 75\n\nEpoch 76/100\nTrain Loss: 0.0014, Validation Loss: 0.0313\nCheckpoint saved at epoch 76\n\nEpoch 77/100\nTrain Loss: 0.0013, Validation Loss: 0.0295\nCheckpoint saved at epoch 77\n\nEpoch 78/100\nTrain Loss: 0.0012, Validation Loss: 0.0327\nCheckpoint saved at epoch 78\n\nEpoch 79/100\nTrain Loss: 0.0013, Validation Loss: 0.0313\nCheckpoint saved at epoch 79\n\nEpoch 80/100\nTrain Loss: 0.0015, Validation Loss: 0.0302\nCheckpoint saved at epoch 80\n\nEpoch 81/100\nTrain Loss: 0.0016, Validation Loss: 0.0308\nCheckpoint saved at epoch 81\n\nEpoch 82/100\nTrain Loss: 0.0014, Validation Loss: 0.0291\nCheckpoint saved at epoch 82\n\nEpoch 83/100\nTrain Loss: 0.0012, Validation Loss: 0.0327\nCheckpoint saved at epoch 83\n\nEpoch 84/100\nTrain Loss: 0.0011, Validation Loss: 0.0358\nCheckpoint saved at epoch 84\n\nEpoch 85/100\nTrain Loss: 0.0011, Validation Loss: 0.0309\nCheckpoint saved at epoch 85\n\nEpoch 86/100\nTrain Loss: 0.0011, Validation Loss: 0.0271\n✅ Saved new best model at epoch 86 with val loss 0.0271\nCheckpoint saved at epoch 86\n\nEpoch 87/100\nTrain Loss: 0.0010, Validation Loss: 0.0279\nCheckpoint saved at epoch 87\n\nEpoch 88/100\nTrain Loss: 0.0011, Validation Loss: 0.0317\nCheckpoint saved at epoch 88\n\nEpoch 89/100\nTrain Loss: 0.0010, Validation Loss: 0.0293\nCheckpoint saved at epoch 89\n\nEpoch 90/100\nTrain Loss: 0.0010, Validation Loss: 0.0303\nCheckpoint saved at epoch 90\n\nEpoch 91/100\nTrain Loss: 0.0011, Validation Loss: 0.0298\nCheckpoint saved at epoch 91\n\nEpoch 92/100\nTrain Loss: 0.0010, Validation Loss: 0.0307\nCheckpoint saved at epoch 92\n\nEpoch 93/100\nTrain Loss: 0.0010, Validation Loss: 0.0315\nCheckpoint saved at epoch 93\n\nEpoch 94/100\nTrain Loss: 0.0010, Validation Loss: 0.0296\nCheckpoint saved at epoch 94\n\nEpoch 95/100\nTrain Loss: 0.0010, Validation Loss: 0.0288\nCheckpoint saved at epoch 95\n\nEpoch 96/100\nTrain Loss: 0.0009, Validation Loss: 0.0264\n✅ Saved new best model at epoch 96 with val loss 0.0264\nCheckpoint saved at epoch 96\n\nEpoch 97/100\nTrain Loss: 0.0010, Validation Loss: 0.0307\nCheckpoint saved at epoch 97\n\nEpoch 98/100\nTrain Loss: 0.0010, Validation Loss: 0.0307\nCheckpoint saved at epoch 98\n\nEpoch 99/100\nTrain Loss: 0.0009, Validation Loss: 0.0296\nCheckpoint saved at epoch 99\n\nEpoch 100/100\nTrain Loss: 0.0009, Validation Loss: 0.0308\nCheckpoint saved at epoch 100\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"# import os\n# import shutil\n\n# # Delete a specific file\n# file_path = '/kaggle/working/extension_model/best_model.pth'\n# if os.path.exists(file_path):\n#     os.remove(file_path)\n# file_path = '/kaggle/working/extension_model/checkpoint.pth'\n# if os.path.exists(file_path):\n#     os.remove(file_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:47:56.441793Z","iopub.execute_input":"2025-05-25T11:47:56.442104Z","iopub.status.idle":"2025-05-25T11:47:56.445922Z","shell.execute_reply.started":"2025-05-25T11:47:56.442079Z","shell.execute_reply":"2025-05-25T11:47:56.444927Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"\n# # Delete all files in /kaggle/working\n# for f in os.listdir('/kaggle/working'):\n#     path = os.path.join('/kaggle/working', f)\n#     if os.path.isfile(path):\n#         os.remove(path)\n#     else:\n#         shutil.rmtree(path)  # If it's a directory\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T11:47:56.447918Z","iopub.execute_input":"2025-05-25T11:47:56.448115Z","iopub.status.idle":"2025-05-25T11:47:56.467029Z","shell.execute_reply.started":"2025-05-25T11:47:56.448099Z","shell.execute_reply":"2025-05-25T11:47:56.466296Z"}},"outputs":[],"execution_count":27}]}