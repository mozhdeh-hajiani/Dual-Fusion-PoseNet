{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyMPwZ1y77TjSYBh4jqddYki"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11858828,"sourceType":"datasetVersion","datasetId":7451592},{"sourceId":11858862,"sourceType":"datasetVersion","datasetId":7451614},{"sourceId":11874387,"sourceType":"datasetVersion","datasetId":7462530},{"sourceId":11914133,"sourceType":"datasetVersion","datasetId":7490253},{"sourceId":11926459,"sourceType":"datasetVersion","datasetId":7498437},{"sourceId":11934122,"sourceType":"datasetVersion","datasetId":7503007},{"sourceId":11935409,"sourceType":"datasetVersion","datasetId":7503807},{"sourceId":11945485,"sourceType":"datasetVersion","datasetId":7509624}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F  # Import for F.mse_loss\nfrom torchvision import models\nimport cv2\nimport yaml\nfrom tqdm import tqdm","metadata":{"id":"IvgMsSv7dunm","executionInfo":{"status":"ok","timestamp":1747388413136,"user_tz":-210,"elapsed":10433,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:04.712574Z","iopub.execute_input":"2025-05-25T12:08:04.712799Z","iopub.status.idle":"2025-05-25T12:08:13.473614Z","shell.execute_reply.started":"2025-05-25T12:08:04.712781Z","shell.execute_reply":"2025-05-25T12:08:13.472680Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install open3d\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HV3Q3nZHsvN","executionInfo":{"status":"ok","timestamp":1747388433735,"user_tz":-210,"elapsed":20584,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"outputId":"daeb40e7-8ed8-4728-d85a-cd1784040961","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:13.474458Z","iopub.execute_input":"2025-05-25T12:08:13.474760Z","iopub.status.idle":"2025-05-25T12:08:33.733450Z","shell.execute_reply.started":"2025-05-25T12:08:13.474740Z","shell.execute_reply":"2025-05-25T12:08:33.732706Z"}},"outputs":[{"name":"stdout","text":"Collecting open3d\n  Downloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.26.4)\nCollecting dash>=2.6.0 (from open3d)\n  Downloading dash-3.0.4-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: werkzeug>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.3)\nRequirement already satisfied: flask>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.1.0)\nRequirement already satisfied: nbformat>=5.7.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (5.10.4)\nCollecting configargparse (from open3d)\n  Downloading configargparse-1.7.1-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: ipywidgets>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from open3d) (8.1.5)\nCollecting addict (from open3d)\n  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\nRequirement already satisfied: pillow>=9.3.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (11.1.0)\nRequirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from open3d) (3.7.2)\nRequirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.11/dist-packages (from open3d) (2.2.3)\nRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from open3d) (6.0.2)\nRequirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.11/dist-packages (from open3d) (1.2.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from open3d) (4.67.1)\nCollecting pyquaternion (from open3d)\n  Downloading pyquaternion-0.9.9-py3-none-any.whl.metadata (1.4 kB)\nCollecting flask>=3.0.0 (from open3d)\n  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\nCollecting werkzeug>=3.0.0 (from open3d)\n  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (5.24.1)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (8.7.0)\nRequirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (4.13.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (2.32.3)\nCollecting retrying (from dash>=2.6.0->open3d)\n  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (1.6.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash>=2.6.0->open3d) (75.2.0)\nRequirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (3.1.6)\nRequirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (8.1.8)\nRequirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from flask>=3.0.0->open3d) (1.9.0)\nRequirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (0.2.2)\nRequirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (7.34.0)\nRequirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (5.7.1)\nRequirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (4.0.14)\nRequirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.11/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.13)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (25.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->open3d) (2.9.0.post0)\nRequirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (2.21.1)\nRequirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (4.23.0)\nRequirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /usr/local/lib/python3.11/dist-packages (from nbformat>=5.7.0->open3d) (5.7.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->open3d) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->open3d) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->open3d) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->open3d) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->open3d) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->open3d) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0->open3d) (2025.2)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21->open3d) (3.6.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=3.0.0->open3d) (3.0.2)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (3.0.50)\nRequirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.19.1)\nRequirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.1.7)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.9.0)\nRequirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (25.3.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (2024.10.1)\nRequirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat>=5.7.0->open3d) (0.24.0)\nRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core!=5.0.*,>=4.12->nbformat>=5.7.0->open3d) (4.3.8)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (9.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.17.0)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash>=2.6.0->open3d) (3.21.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->open3d) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->open3d) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.0->open3d) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.0->open3d) (2024.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash>=2.6.0->open3d) (2025.4.26)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.0->open3d) (2024.2.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.4)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.13)\nDownloading open3d-0.19.0-cp311-cp311-manylinux_2_31_x86_64.whl (447.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m447.7/447.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading dash-3.0.4-py3-none-any.whl (7.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m107.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading addict-2.4.0-py3-none-any.whl (3.8 kB)\nDownloading configargparse-1.7.1-py3-none-any.whl (25 kB)\nDownloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\nDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\nInstalling collected packages: addict, werkzeug, retrying, configargparse, flask, dash, pyquaternion, open3d\n  Attempting uninstall: werkzeug\n    Found existing installation: Werkzeug 3.1.3\n    Uninstalling Werkzeug-3.1.3:\n      Successfully uninstalled Werkzeug-3.1.3\n  Attempting uninstall: flask\n    Found existing installation: Flask 3.1.0\n    Uninstalling Flask-3.1.0:\n      Successfully uninstalled Flask-3.1.0\nSuccessfully installed addict-2.4.0 configargparse-1.7.1 dash-3.0.4 flask-3.0.3 open3d-0.19.0 pyquaternion-0.9.9 retrying-1.3.4 werkzeug-3.0.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport cv2\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\nimport torchvision.transforms as T\nimport os\nimport yaml\nfrom PIL import Image\n\n\nclass PoseDataset(Dataset):\n    def __init__(self,  rgb_dir, depth_dir,linemod_root, augment=False):\n\n        self.rgb_dir = rgb_dir\n        self.depth_dir = depth_dir\n        self.linemod_root = linemod_root\n        self.RGB_img_filenames = sorted([\n            f for f in os.listdir(rgb_dir) if f.endswith(\".png\")\n        ])\n        self.depth_img_filenames = sorted([\n            f for f in os.listdir(depth_dir) if f.endswith(\".png\")\n        ])\n\n\n        # Preload gt.yml data for all classes\n        self.gt_data = {}\n        for class_id in range(1, 16):\n            class_str = f\"{class_id:02d}\"\n            gt_path = os.path.join(linemod_root, class_str, \"gt.yml\")\n            if os.path.exists(gt_path):\n                with open(gt_path, 'r') as f:\n                    self.gt_data[class_str] = yaml.safe_load(f)\n\n        \n        self.rgb_transform = T.Compose([\n            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            T.RandomHorizontalFlip(),\n            T.Resize((224, 224)),\n            T.ToTensor(),\n            T.Normalize(mean=[0.485, 0.456, 0.406],\n            std=[0.229, 0.224, 0.225])\n        ])\n        self.depth_transform = T.Compose([\n            # T.ToTensor(),\n            # T.Resize((224, 224)),\n            T.Resize((224, 224)),\n            T.ToTensor()  # Converts PIL float32 to FloatTensor [1, H, W]\n        ])\n\n    def __len__(self):\n        return len(self.RGB_img_filenames)\n\n    def normalize_depth(self,depth):\n        depth = np.array(depth).astype(np.float32)\n        return (depth - depth.min()) / (depth.max() - depth.min() + 1e-8)\n\n    def __getitem__(self, idx):\n\n        RGB_filename = self.RGB_img_filenames[idx]\n        class_id_str, img_id_str = RGB_filename.split(\"_\")\n        img_id = int(os.path.splitext(img_id_str)[0])\n\n        # Load RGB image\n        rgb_path = os.path.join(self.rgb_dir, RGB_filename)\n        rgb_img = Image.open(rgb_path).convert(\"RGB\")\n        rgb_tensor = self.rgb_transform(rgb_img)\n\n        # Load Depth image\n        # depth_path = os.path.join(self.depth_dir, RGB_filename)\n        # depth_img = Image.open(depth_path).convert(\"I\")   # Single-channel\n\n        # # Normalize and convert to PIL for transforms\n        # depth_np_norm = self.normalize_depth(depth_img)\n        # depth_img_norm = Image.fromarray((depth_np_norm * 255).astype(np.uint8))\n\n        # depth_tensor = self.depth_transform(depth_img_norm)\n           # Load Depth image (PIL single channel)\n        depth_path = os.path.join(self.depth_dir, RGB_filename)\n        depth_img = Image.open(depth_path).convert(\"I\")  # 32-bit integer depth\n\n        # Convert to float numpy, normalize (e.g., scale mm->meters or divide by max)\n        depth_np = np.array(depth_img).astype(np.float32)\n        depth_np /= 1000.0  # if in mm, convert to meters, adjust as per your data\n\n        # Optional: clip depth values to a max distance (e.g., 2 meters)\n        depth_np = np.clip(depth_np, 0, 2.0)\n\n        # Normalize depth to [0,1] by dividing by max depth value (2.0)\n        depth_np /= 2.0\n\n        # Convert normalized float depth to PIL image in 'F' mode (32-bit float)\n        depth_img_float = Image.fromarray(depth_np).convert('F')\n\n        # Apply depth transforms (Resize -> ToTensor)\n        depth_tensor = self.depth_transform(depth_img_float)  # [1, H, W], float32 in [0,1]\n\n\n        # Load pose from GT file\n        pose_list = self.gt_data[class_id_str][img_id]\n        pose = next(item for item in pose_list if item['obj_id'] == int(class_id_str))\n\n        R_mat = np.array(pose['cam_R_m2c']).reshape(3, 3).astype(np.float32)\n        quat = R.from_matrix(R_mat).as_quat().astype(np.float32)\n        quat /= np.linalg.norm(quat)\n        t_vec = np.array(pose['cam_t_m2c'], dtype=np.float32) / 1000.0  #  to meters\n\n        return {\n            'RGB_image': rgb_tensor,\n            'depth_image': depth_tensor,\n            'rotation': torch.tensor(quat, dtype=torch.float32),\n            'rotation_matrix': torch.tensor(R_mat, dtype=torch.float32),\n            'translation': torch.tensor(t_vec, dtype=torch.float32),\n            'class_id': int(class_id_str),\n            'filename': RGB_filename\n        }\n\n\n","metadata":{"id":"V3L-8NAETik1","executionInfo":{"status":"ok","timestamp":1747388433864,"user_tz":-210,"elapsed":82,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:33.735356Z","iopub.execute_input":"2025-05-25T12:08:33.735599Z","iopub.status.idle":"2025-05-25T12:08:34.018922Z","shell.execute_reply.started":"2025-05-25T12:08:33.735575Z","shell.execute_reply":"2025-05-25T12:08:34.018148Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass PoseNet6D_MLP_ConcatFusion(nn.Module):\n    def __init__(self, pretrained=True, compress_rgb=True):\n        super(PoseNet6D_MLP_ConcatFusion, self).__init__()\n\n        # RGB branch (ResNet50)\n        resnet_rgb = models.resnet50(\n            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n        )\n        self.rgb_backbone = nn.Sequential(*list(resnet_rgb.children())[:-1])  # (B, 2048, 1, 1)\n\n        # Compress RGB features to 512-dim\n        self.compress_rgb = compress_rgb\n        if compress_rgb:\n            self.rgb_compress = nn.Linear(2048, 512)\n\n        # Depth branch (ResNet18, 1-channel input)\n        resnet_depth = models.resnet18(weights=None)\n        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.depth_backbone = nn.Sequential(*list(resnet_depth.children())[:-1])  # (B, 512, 1, 1)\n\n        # MLP gating to produce 2 gating scalars\n        self.gate_mlp = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2),\n            nn.Sigmoid()\n        )\n\n        # Pose regression layers (input = 1024 after concat)\n        self.fc_rot = nn.Linear(1024, 4)\n        self.fc_trans = nn.Linear(1024, 3)\n\n    def forward(self, rgb_img, depth_img):\n        # RGB feature extraction\n        rgb_feat = self.rgb_backbone(rgb_img).squeeze(-1).squeeze(-1)  # (B, 2048)\n        if self.compress_rgb:\n            rgb_feat = self.rgb_compress(rgb_feat)  # (B, 512)\n\n        # Depth feature extraction\n        depth_feat = self.depth_backbone(depth_img).squeeze(-1).squeeze(-1)  # (B, 512)\n\n        # Gating\n        concat_feat = torch.cat([rgb_feat, depth_feat], dim=1)  # (B, 1024)\n        gates = self.gate_mlp(concat_feat)                      # (B, 2)\n        rgb_gate = gates[:, 0].unsqueeze(1)\n        depth_gate = gates[:, 1].unsqueeze(1)\n\n        # Apply gates\n        gated_rgb_feat = rgb_feat * rgb_gate\n        gated_depth_feat = depth_feat * depth_gate\n\n        # Fusion by concatenation (final feature = 1024)\n        fused_feat = torch.cat([gated_rgb_feat, gated_depth_feat], dim=1)  # (B, 1024)\n\n        # Pose regression\n        rot = self.fc_rot(fused_feat)\n        trans = self.fc_trans(fused_feat)\n        rot = F.normalize(rot, dim=1)\n\n        return rot, trans\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:34.019683Z","iopub.execute_input":"2025-05-25T12:08:34.020094Z","iopub.status.idle":"2025-05-25T12:08:34.028586Z","shell.execute_reply.started":"2025-05-25T12:08:34.020076Z","shell.execute_reply":"2025-05-25T12:08:34.027885Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass PoseNet6D_MLP_ConcatFusion_512(nn.Module):\n    def __init__(self, pretrained=True, compress_rgb=True):\n        super(PoseNet6D_MLP_ConcatFusion_512, self).__init__()\n\n        # RGB branch (ResNet50)\n        resnet_rgb = models.resnet50(\n            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n        )\n        self.rgb_backbone = nn.Sequential(*list(resnet_rgb.children())[:-1])  # (B, 2048, 1, 1)\n\n        # Compress RGB features to 512-dim\n        self.compress_rgb = compress_rgb\n        if compress_rgb:\n            self.rgb_compress = nn.Linear(2048, 512)\n\n        # Depth branch (ResNet18, 1-channel input)\n        resnet_depth = models.resnet18(weights=None)\n        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.depth_backbone = nn.Sequential(*list(resnet_depth.children())[:-1])  # (B, 512, 1, 1)\n\n        # MLP gating to produce 2 gating scalars\n        self.gate_mlp = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2),\n            nn.Sigmoid()\n        )\n\n        # Fusion projection layer after concatenation (1024 → 512)\n        self.fusion_fc = nn.Linear(1024, 512)\n\n        # Pose regression layers\n        self.fc_rot = nn.Linear(512, 4)\n        self.fc_trans = nn.Linear(512, 3)\n\n    def forward(self, rgb_img, depth_img):\n        # RGB feature extraction\n        rgb_feat = self.rgb_backbone(rgb_img).squeeze(-1).squeeze(-1)  # (B, 2048)\n        if self.compress_rgb:\n            rgb_feat = self.rgb_compress(rgb_feat)  # (B, 512)\n\n        # Depth feature extraction\n        depth_feat = self.depth_backbone(depth_img).squeeze(-1).squeeze(-1)  # (B, 512)\n\n        # Gating\n        concat_feat = torch.cat([rgb_feat, depth_feat], dim=1)  # (B, 1024)\n        gates = self.gate_mlp(concat_feat)                      # (B, 2)\n        rgb_gate = gates[:, 0].unsqueeze(1)\n        depth_gate = gates[:, 1].unsqueeze(1)\n\n        # Apply gates\n        gated_rgb_feat = rgb_feat * rgb_gate\n        gated_depth_feat = depth_feat * depth_gate\n\n        # Fusion by concatenation\n        fused_feat = torch.cat([gated_rgb_feat, gated_depth_feat], dim=1)  # (B, 1024)\n        fused_feat = self.fusion_fc(fused_feat)  # (B, 512)\n\n        # Pose regression\n        rot = self.fc_rot(fused_feat)\n        trans = self.fc_trans(fused_feat)\n        rot = F.normalize(rot, dim=1)\n\n        return rot, trans\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:34.029156Z","iopub.execute_input":"2025-05-25T12:08:34.029373Z","iopub.status.idle":"2025-05-25T12:08:34.051493Z","shell.execute_reply.started":"2025-05-25T12:08:34.029357Z","shell.execute_reply":"2025-05-25T12:08:34.050784Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass PoseNet6D_MLP_Fusion(nn.Module):\n    def __init__(self, pretrained=True, compress_rgb=True):\n        super(PoseNet6D_MLP_Fusion, self).__init__()\n\n        # RGB branch (ResNet50)\n        resnet_rgb = models.resnet50(\n            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n        )\n        self.rgb_backbone = nn.Sequential(*list(resnet_rgb.children())[:-1])  # (B, 2048, 1, 1)\n\n        # Compress RGB features to 512-dim\n        self.compress_rgb = compress_rgb\n        if compress_rgb:\n            self.rgb_compress = nn.Linear(2048, 512)\n\n        # Depth branch (ResNet18, 1-channel input)\n        resnet_depth = models.resnet18(weights=None)\n        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.depth_backbone = nn.Sequential(*list(resnet_depth.children())[:-1])  # (B, 512, 1, 1)\n\n        # MLP gating to produce 2 gating scalars (for RGB and Depth) from concatenated features\n        # Input size = 512 + 512 = 1024 (after compression)\n        self.gate_mlp = nn.Sequential(\n            nn.Linear(1024, 256),\n            nn.ReLU(),\n            nn.Linear(256, 2),      # 2 gating weights: one for RGB, one for Depth\n            nn.Sigmoid()            # Output in [0,1]\n        )\n\n        # Pose regression layers (after fusion)\n        self.fc_rot = nn.Linear(512, 4)   # quaternion\n        self.fc_trans = nn.Linear(512, 3) # translation\n\n    def forward(self, rgb_img, depth_img):\n        # RGB feature extraction\n        rgb_feat = self.rgb_backbone(rgb_img).squeeze(-1).squeeze(-1)  # (B, 2048)\n        if self.compress_rgb:\n            rgb_feat = self.rgb_compress(rgb_feat)  # (B, 512)\n\n        # Depth feature extraction\n        depth_feat = self.depth_backbone(depth_img).squeeze(-1).squeeze(-1)  # (B, 512)\n\n        # Concatenate features for gating MLP\n        concat_feat = torch.cat([rgb_feat, depth_feat], dim=1)  # (B, 1024)\n        gates = self.gate_mlp(concat_feat)                      # (B, 2), values in [0,1]\n\n        # Split gating weights\n        rgb_gate = gates[:, 0].unsqueeze(1)     # (B,1)\n        depth_gate = gates[:, 1].unsqueeze(1)   # (B,1)\n\n        # Apply gates to features (element-wise multiply)\n        gated_rgb_feat = rgb_feat * rgb_gate    # (B, 512)\n        gated_depth_feat = depth_feat * depth_gate  # (B, 512)\n\n        # Fuse gated features (sum or concat; here we sum)\n        fused_feat = gated_rgb_feat + gated_depth_feat  # (B, 512)\n\n        # Pose regression\n        rot = self.fc_rot(fused_feat)       # (B, 4)\n        trans = self.fc_trans(fused_feat)   # (B, 3)\n\n        # Normalize quaternion to unit length\n        rot = F.normalize(rot, dim=1)\n\n        return rot, trans\n","metadata":{"id":"ZS0gQTlOTYet","executionInfo":{"status":"ok","timestamp":1747388433983,"user_tz":-210,"elapsed":51,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:34.052296Z","iopub.execute_input":"2025-05-25T12:08:34.052543Z","iopub.status.idle":"2025-05-25T12:08:34.072034Z","shell.execute_reply.started":"2025-05-25T12:08:34.052521Z","shell.execute_reply":"2025-05-25T12:08:34.071511Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass PoseNet6D(nn.Module):\n    def __init__(self, pretrained=True, compress_rgb=True):\n        super(PoseNet6D, self).__init__()\n\n        # ==== RGB branch: ResNet50 ====\n        resnet_rgb = models.resnet50(\n            weights=models.ResNet50_Weights.IMAGENET1K_V2 if pretrained else None\n        )\n        self.rgb_backbone = nn.Sequential(*list(resnet_rgb.children())[:-1])  # Output: (B, 2048, 1, 1)\n\n        #  compression layer for RGB features\n        self.compress_rgb = compress_rgb\n        if compress_rgb:\n            self.rgb_compress = nn.Linear(2048, 512)  # Match depth feature dim\n\n        # ==== Depth branch: ResNet18 modified for 1-channel input ====\n        resnet_depth = models.resnet18(weights=None)  # No pretrained weights for depth\n        resnet_depth.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.depth_backbone = nn.Sequential(*list(resnet_depth.children())[:-1])  # Output: (B, 512, 1, 1)\n\n        # ==== Pose regression ====\n        self.fc_rot = nn.Linear(1024, 4)   # 512 + 512 -> Quaternion\n        self.fc_trans = nn.Linear(1024, 3) # 512 + 512 -> Translation\n\n    def forward(self, rgb_img, depth_img):\n        # RGB features\n        rgb_feat = self.rgb_backbone(rgb_img).squeeze(-1).squeeze(-1)  # (B, 2048)\n        if self.compress_rgb:\n            rgb_feat = self.rgb_compress(rgb_feat)  # (B, 512)\n\n        # Depth features\n        depth_feat = self.depth_backbone(depth_img).squeeze(-1).squeeze(-1)  # (B, 512)\n\n        # Concatenate features\n        feat = torch.cat([rgb_feat, depth_feat], dim=1)  # (B, 1024)\n\n        # Predict pose\n        rot = self.fc_rot(feat)         # (B, 4)\n        trans = self.fc_trans(feat)     # (B, 3)\n        rot = F.normalize(rot, dim=1)   # Normalize quaternion\n        return rot, trans\n","metadata":{"id":"IUcsF6Mrd4zO","executionInfo":{"status":"ok","timestamp":1747388434016,"user_tz":-210,"elapsed":27,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:34.072789Z","iopub.execute_input":"2025-05-25T12:08:34.073043Z","iopub.status.idle":"2025-05-25T12:08:34.096255Z","shell.execute_reply.started":"2025-05-25T12:08:34.073021Z","shell.execute_reply":"2025-05-25T12:08:34.095629Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef mse_pose_loss(pred_q, pred_t, gt_q, gt_t):\n    return torch.mean((pred_q - gt_q)**2) + torch.mean((pred_t - gt_t)**2)\n\ndef angle_pose_loss(pred_q, pred_t, gt_q, gt_t):\n    pred_q = F.normalize(pred_q, dim=1)\n    gt_q = F.normalize(gt_q, dim=1)\n    cos_sim = torch.sum(pred_q * gt_q, dim=1).clamp(-1+1e-7, 1-1e-7)\n    angle_loss = torch.mean(1 - cos_sim.abs())\n    trans_loss = torch.mean((pred_t - gt_t)**2)\n    return angle_loss + trans_loss\n\ndef smooth_l1_pose_loss(pred_q, pred_t, gt_q, gt_t):\n    return F.smooth_l1_loss(pred_q, gt_q) + F.smooth_l1_loss(pred_t, gt_t)\n\ndef pose_loss(pred_q, pred_t, gt_q, gt_t):\n    rot_loss = 1 - torch.sum(pred_q * gt_q, dim=1)**2\n    trans_loss = torch.mean((pred_t - gt_t)**2, dim=1)\n    return rot_loss.mean() + trans_loss.mean()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:34.097005Z","iopub.execute_input":"2025-05-25T12:08:34.097261Z","iopub.status.idle":"2025-05-25T12:08:34.116310Z","shell.execute_reply.started":"2025-05-25T12:08:34.097234Z","shell.execute_reply":"2025-05-25T12:08:34.115600Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def train_model(model, dataloader, optimizer, device, scaler=None):\n    model.train()\n    total_loss = 0.0\n\n    for batch in dataloader:\n        rgb = batch['RGB_image'].to(device)\n        depth = batch['depth_image'].to(device)\n        gt_q = batch['rotation'].to(device)\n        gt_t = batch['translation'].to(device)\n\n        optimizer.zero_grad()\n\n        if scaler:  # Mixed precision\n            with torch.cuda.amp.autocast():\n                pred_q, pred_t = model(rgb, depth)\n                loss = pose_loss(pred_q, pred_t, gt_q, gt_t)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            pred_q, pred_t = model(rgb, depth)\n            loss = pose_loss(pred_q, pred_t, gt_q, gt_t)\n               # Debug print (only for the first batch)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # optional\n            optimizer.step()\n        # Print per-batch loss\n        print(f\"[Batch {i+1}/{len(dataloader)}] Loss: {loss.item():.4f}\")\n        total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n","metadata":{"id":"r8uyfh_b84Sl","executionInfo":{"status":"ok","timestamp":1747388434035,"user_tz":-210,"elapsed":3,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:34.118486Z","iopub.execute_input":"2025-05-25T12:08:34.118875Z","iopub.status.idle":"2025-05-25T12:08:34.136322Z","shell.execute_reply.started":"2025-05-25T12:08:34.118858Z","shell.execute_reply":"2025-05-25T12:08:34.135633Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def validate_model(model, dataloader, device):\n    model.eval()\n    total_loss = 0.0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            rgb = batch['RGB_image'].to(device)\n            depth = batch['depth_image'].to(device)\n            gt_q = batch['rotation'].to(device)\n            gt_t = batch['translation'].to(device)\n\n            pred_q, pred_t = model(rgb, depth)\n            loss = pose_loss(pred_q, pred_t, gt_q, gt_t)\n            total_loss += loss.item()\n\n    return total_loss / len(dataloader)\n","metadata":{"id":"poq9Eb3J88E3","executionInfo":{"status":"ok","timestamp":1747388434040,"user_tz":-210,"elapsed":1,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:34.136909Z","iopub.execute_input":"2025-05-25T12:08:34.137148Z","iopub.status.idle":"2025-05-25T12:08:34.151398Z","shell.execute_reply.started":"2025-05-25T12:08:34.137128Z","shell.execute_reply":"2025-05-25T12:08:34.150813Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"from scipy.spatial.transform import Rotation as R\nimport numpy as np\nimport open3d as o3d\nfrom tqdm import tqdm\nimport os\nfrom collections import defaultdict\n\n\ndef load_model_points(models_dir, class_id):\n    class_id = int(class_id)  # Ensure it's an integer\n    model_path = os.path.join(models_dir, f\"obj_{class_id:02d}.ply\")\n    if not os.path.exists(model_path):\n        print(f\"[ERROR] File not found: {model_path}\")\n    mesh = o3d.io.read_triangle_mesh(model_path)\n    return np.asarray(mesh.vertices).astype(np.float32)\n\n\ndef compute_ADD(R_pred, t_pred, R_gt, t_gt, model_points):\n    pred_pts = (R_pred @ model_points.T).T + t_pred\n    gt_pts = (R_gt @ model_points.T).T + t_gt\n    return np.mean(np.linalg.norm(pred_pts - gt_pts, axis=1))\n\ndef evaluate_ADD_per_class(model, dataloader, device, threshold=0.1):\n    model.eval()\n\n    models_dir = os.path.join(\"/kaggle/input/linemod/Linemod_preprocessed\", \"models\")\n\n    add_scores_per_class = defaultdict(list)\n    correct_counts_per_class = defaultdict(int)\n    total_counts_per_class = defaultdict(int)\n\n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating ADD per class\"):\n            rgb = batch['RGB_image'].to(device)\n            depth = batch['depth_image'].to(device)\n            gt_q = batch['rotation'].cpu().numpy()\n            gt_t = batch['translation'].cpu().numpy()\n            class_ids = batch['class_id'].cpu().numpy()\n\n            pred_q, pred_t = model(rgb, depth)\n            pred_q = pred_q.cpu().numpy()\n            pred_t = pred_t.cpu().numpy()\n\n            # Normalize predicted quaternions\n            norms = np.linalg.norm(pred_q, axis=1, keepdims=True) + 1e-8  # avoid division by zero\n            pred_q = pred_q / norms\n\n            for i in range(len(gt_q)):\n                class_id = class_ids[i]\n                R_gt = R.from_quat(gt_q[i]).as_matrix()\n                R_pred = R.from_quat(pred_q[i]).as_matrix()\n                t_gt = gt_t[i]\n                t_pred = pred_t[i]\n\n                model_points = load_model_points(models_dir, class_id)\n                model_points = model_points / 1000.0\n                add = compute_ADD(R_pred, t_pred, R_gt, t_gt, model_points)\n\n                add_scores_per_class[class_id].append(add)\n                total_counts_per_class[class_id] += 1\n                if add < threshold:\n                    correct_counts_per_class[class_id] += 1\n\n    print(\"\\n=== ADD per class ===\")\n    mean_adds = []\n    for class_id in sorted(add_scores_per_class.keys()):\n        mean_add = np.mean(add_scores_per_class[class_id])\n        mean_adds.append(mean_add)\n        acc = 100.0 * correct_counts_per_class[class_id] / total_counts_per_class[class_id]\n        print(f\"Class {class_id:02d} → Mean ADD: {mean_add:.4f} m, Accuracy (<{threshold*100:.0f}cm): {acc:.2f}%\")\n    overall_mean_add = np.mean(mean_adds)\n    print(f\"\\n=== Overall Mean ADD (averaged over classes): {overall_mean_add:.4f} m ===\")\n\n    return add_scores_per_class, correct_counts_per_class, total_counts_per_class,overall_mean_add","metadata":{"id":"F_Jfe52UFY_Y","executionInfo":{"status":"ok","timestamp":1747388435167,"user_tz":-210,"elapsed":1117,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:34.152086Z","iopub.execute_input":"2025-05-25T12:08:34.152364Z","iopub.status.idle":"2025-05-25T12:08:35.664437Z","shell.execute_reply.started":"2025-05-25T12:08:34.152313Z","shell.execute_reply":"2025-05-25T12:08:35.663766Z"}},"outputs":[{"name":"stdout","text":"Jupyter environment detected. Enabling Open3D WebVisualizer.\n[Open3D INFO] WebRTC GUI backend enabled.\n[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch\nimport random\nimport numpy as np\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)  # if using multi-GPU\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:08:35.665229Z","iopub.execute_input":"2025-05-25T12:08:35.665670Z","iopub.status.idle":"2025-05-25T12:08:35.674736Z","shell.execute_reply.started":"2025-05-25T12:08:35.665644Z","shell.execute_reply":"2025-05-25T12:08:35.674086Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nimport random\nimport shutil\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\nRGB_cropped_dir = \"/kaggle/input/rgboutput/RGB_crop/train/train_cropped_objects\"\ndepth_cropped_dir = \"/kaggle/input/depthoutput/depth_crop/train/train_cropped_objects\"\nlinemod_root = \"/kaggle/input/linemod/Linemod_preprocessed/data\"\nsave_path = \"/kaggle/working/test_dataset.pt\"\nset_seed(42)\n# Load dataset\nRGB_image_files = [f for f in os.listdir(RGB_cropped_dir) if f.endswith(\".png\")]\n\ntest_dataset = PoseDataset(\n    rgb_dir=RGB_cropped_dir,\n    depth_dir=depth_cropped_dir,\n    linemod_root=linemod_root\n)\nprint(f\"len of test_dataset is={len(test_dataset)}\")\ntorch.save(test_dataset, save_path)\nprint(f\"test_dataset saved at: {save_path}\")\n\n\n\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\nprint(\"test loader done\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = PoseNet6D_MLP_ConcatFusion() #PoseNet6D_MLP_ConcatFusion   PoseNet6D_MLP_Fusion\nmodel.to(device)\n# Load model weights\nmodel.load_state_dict(torch.load(\"/kaggle/input/loss-dataset/mse-loss/best_model.pth\"))\nprint(\"model loading done\")\nmodel.to(device)\nmodel.eval()\n# Evaluation\nwith torch.no_grad():\n    add_scores_per_class, correct_counts_per_class, total_counts_per_class, overall_mean_add = evaluate_ADD_per_class(\n        model, test_loader, device\n    )\n# add_scores_per_class, correct_counts_per_class, total_counts_per_class,overall_mean_add= evaluate_ADD_per_class(model, test_loader, device)\n\n\n","metadata":{"id":"b5dDo21y1ZCm","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:17:11.227791Z","iopub.execute_input":"2025-05-25T12:17:11.228060Z","iopub.status.idle":"2025-05-25T12:19:12.190413Z","shell.execute_reply.started":"2025-05-25T12:17:11.228040Z","shell.execute_reply":"2025-05-25T12:19:12.189795Z"}},"outputs":[{"name":"stdout","text":"len of test_dataset is=2373\ntest_dataset saved at: /kaggle/working/test_dataset.pt\ntest loader done\nmodel loading done\n","output_type":"stream"},{"name":"stderr","text":"Evaluating ADD per class: 100%|██████████| 149/149 [01:35<00:00,  1.56it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n=== ADD per class ===\nClass 01 → Mean ADD: 0.0552 m, Accuracy (<10cm): 90.86%\nClass 02 → Mean ADD: 0.0372 m, Accuracy (<10cm): 95.58%\nClass 04 → Mean ADD: 0.0428 m, Accuracy (<10cm): 93.37%\nClass 05 → Mean ADD: 0.0433 m, Accuracy (<10cm): 94.44%\nClass 06 → Mean ADD: 0.0451 m, Accuracy (<10cm): 90.96%\nClass 08 → Mean ADD: 0.0427 m, Accuracy (<10cm): 91.62%\nClass 09 → Mean ADD: 0.0525 m, Accuracy (<10cm): 89.95%\nClass 10 → Mean ADD: 0.0516 m, Accuracy (<10cm): 88.83%\nClass 11 → Mean ADD: 0.0459 m, Accuracy (<10cm): 95.11%\nClass 12 → Mean ADD: 0.0444 m, Accuracy (<10cm): 91.94%\nClass 13 → Mean ADD: 0.0475 m, Accuracy (<10cm): 90.75%\nClass 14 → Mean ADD: 0.0432 m, Accuracy (<10cm): 91.35%\nClass 15 → Mean ADD: 0.0408 m, Accuracy (<10cm): 95.11%\n\n=== Overall Mean ADD (averaged over classes): 0.0456 m ===\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# test_dataset = torch.load(\"/content/drive/MyDrive/test_dataset.pt\", weights_only=False)\n# print(\"test_dataset loaded.\")\n# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)\n# print(\"test loader done\")\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = PoseNet6D_MLP_GatingFusion()\n# model.to(device)\n# # Load model weights\n# model.load_state_dict(torch.load(\"/content/drive/MyDrive/yolo_models/linemod_yolo_v8n/extension_model_all/final_model.pth\"))\n# print(\"model loading done\")\n# model.to(device)\n# add_scores_per_class, correct_counts_per_class, total_counts_per_class,overall_mean_add= evaluate_ADD_per_class(model, test_loader, device)\n\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T3-LMS4MBwJo","outputId":"b45445d7-15f1-4b08-e17a-e933e177cafb","executionInfo":{"status":"ok","timestamp":1747395318546,"user_tz":-210,"elapsed":6774352,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:11:01.430941Z","iopub.execute_input":"2025-05-25T12:11:01.431187Z","iopub.status.idle":"2025-05-25T12:11:01.435331Z","shell.execute_reply.started":"2025-05-25T12:11:01.431168Z","shell.execute_reply":"2025-05-25T12:11:01.434502Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# from scipy.spatial.transform import Rotation as R\n# import numpy as np\n# import open3d as o3d\n# from tqdm import tqdm\n# import os\n# from collections import defaultdict\n# mean_adds = []\n# for class_id in sorted(add_scores_per_class.keys()):\n#     mean_add = np.mean(add_scores_per_class[class_id])\n#     mean_adds.append(mean_add)\n#     acc = 100.0 * correct_counts_per_class[class_id] / total_counts_per_class[class_id]\n#     print(f\"Class {class_id:02d} → Mean ADD: {mean_add:.4f} m\")\n# overall_mean_add = np.mean(mean_adds)\n# print(f\"\\n=== Overall Mean ADD (averaged over classes): {overall_mean_add:.4f} m ===\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uwUSgY_XiMku","executionInfo":{"status":"ok","timestamp":1747373097280,"user_tz":-210,"elapsed":38,"user":{"displayName":"Mozhdeh Mozhdeh","userId":"04482485733436676807"}},"outputId":"9f0c0ca1-ab2c-4736-af62-3e9be6fae28c","trusted":true,"execution":{"iopub.status.busy":"2025-05-25T12:11:01.436447Z","iopub.execute_input":"2025-05-25T12:11:01.436759Z","iopub.status.idle":"2025-05-25T12:11:01.454804Z","shell.execute_reply.started":"2025-05-25T12:11:01.436733Z","shell.execute_reply":"2025-05-25T12:11:01.454126Z"}},"outputs":[],"execution_count":15}]}